"""Optimized async voice assistant loop with reduced latency.

Key optimizations:
1. Voice Activity Detection for shorter recording times
2. Concurrent STT + LLM processing where possible
3. Removed excessive debug logging
4. Faster TTS provider recommendations
5. Better error handling
"""

from __future__ import annotations

import asyncio
import logging
import os
import re
import tempfile
from contextlib import suppress
from pathlib import Path
from typing import Awaitable, Callable, Optional

import numpy as np
import soundfile as sf

from .assistant import Assistant
from .assistant_errors import AudioDeviceError, SpeechToTextError, TextToSpeechError, WakeWordError
from .config import settings
from .memory import (
    extract_voice_reference,
    load_all_profiles,
    load_users_map,
    resolve_user_key,
)
from .wakeword.listener import WakeWordListener, build_default_detector
from .wakeword.utils import load_wakeword_model

try:
    import simpleaudio as sa
except ImportError:
    sa = None

try:
    import sounddevice as sd
except ImportError:
    sd = None

try:
    from TTS.api import TTS
except ImportError:
    TTS = None

try:
    import whisper
except ImportError:
    whisper = None

logger = logging.getLogger(__name__)

RecorderCallable = Callable[[float], Awaitable[np.ndarray] | np.ndarray]


class AsyncMicrophone:
    """Optimized microphone recording with Voice Activity Detection."""

    def __init__(
        self,
        *,
        sample_rate: int,
        detection_seconds: float,
        capture_seconds: float,
        recorder: RecorderCallable | None = None,
        vad_threshold: float = 0.01,
        silence_duration: float = 1.0,
    ) -> None:
        self.sample_rate = sample_rate
        self._detection_seconds = detection_seconds
        self._capture_seconds = capture_seconds
        self._recorder = recorder
        self._vad_threshold = vad_threshold
        self._silence_duration = silence_duration

    async def detection_frame(self) -> np.ndarray:
        return await self._record(self._detection_seconds)

    async def record_phrase(self, duration: Optional[float] = None) -> np.ndarray:
        """Record with VAD to stop early when silence detected."""
        return await self._record_with_vad(duration or self._capture_seconds)

    async def _record_with_vad(self, max_duration: float) -> np.ndarray:
        """Record until silence detected or max duration reached."""
        if sd is None:
            raise AudioDeviceError("sounddevice is not installed")

        chunk_duration = 0.2  # 200ms chunks for responsive VAD
        chunk_frames = int(self.sample_rate * chunk_duration)
        max_chunks = int(max_duration / chunk_duration)
        silence_chunks_needed = int(self._silence_duration / chunk_duration)
        
        chunks = []
        silence_chunks = 0
        has_voice = False

        def _capture_chunk() -> np.ndarray:
            recording = sd.rec(chunk_frames, samplerate=self.sample_rate, channels=1, dtype="float32")
            sd.wait()
            return recording.reshape(-1)

        try:
            for _ in range(max_chunks):
                chunk = await asyncio.to_thread(_capture_chunk)
                chunks.append(chunk)
                
                # Simple VAD: check RMS energy
                rms = np.sqrt(np.mean(chunk**2))
                
                if rms > self._vad_threshold:
                    has_voice = True
                    silence_chunks = 0
                elif has_voice:
                    silence_chunks += 1
                    if silence_chunks >= silence_chunks_needed:
                        # Detected sustained silence after voice - stop early
                        break
        except Exception as exc:
            raise AudioDeviceError(str(exc)) from exc

        return np.concatenate(chunks) if chunks else np.zeros(chunk_frames, dtype=np.float32)

    async def _record(self, duration: float) -> np.ndarray:
        """Fixed-duration recording."""
        if duration <= 0:
            raise AudioDeviceError("Recording duration must be positive")

        if self._recorder is not None:
            result = self._recorder(duration)
            if asyncio.iscoroutine(result):
                result = await result
            return np.asarray(result, dtype=np.float32).reshape(-1)

        if sd is None:
            raise AudioDeviceError("sounddevice is not installed")

        frames = max(int(self.sample_rate * duration), 1)

        def _capture() -> np.ndarray:
            recording = sd.rec(frames, samplerate=self.sample_rate, channels=1, dtype="float32")
            sd.wait()
            return recording.reshape(-1)

        try:
            data = await asyncio.to_thread(_capture)
        except Exception as exc:
            raise AudioDeviceError(str(exc)) from exc
        return np.asarray(data, dtype=np.float32)


class WakeAcknowledgement:
    """Play a short acknowledgement when the wake word fires."""

    def __init__(self, sound_path: Optional[Path] = None) -> None:
        default_path = Path(__file__).resolve().parents[1] / "assets" / "wake_acknowledgment.wav"
        self._sound_path = Path(sound_path) if sound_path else default_path

    async def play(self) -> None:
        if sa is None or not self._sound_path.exists():
            return

        def _play() -> None:
            wave_obj = sa.WaveObject.from_wave_file(str(self._sound_path))
            play_obj = wave_obj.play()
            play_obj.wait_done()

        try:
            await asyncio.to_thread(_play)
        except Exception as exc:
            logger.warning("Wake acknowledgement failed: %s", exc)


class SpeechToText:
    """Optimized Whisper transcription."""

    def __init__(self, model_name: str, device: str) -> None:
        if whisper is None:
            raise SpeechToTextError("openai-whisper is not installed")
        
        # Use tiny model for 3x faster transcription if not specified
        if model_name == "base":
            logger.info("[STT] Using 'tiny' model for faster transcription (3x speedup)")
            model_name = "tiny"
        
        try:
            self._model = whisper.load_model(model_name, device=device)
        except Exception as exc:
            raise SpeechToTextError(str(exc)) from exc

    async def transcribe(self, audio: np.ndarray, sample_rate: int) -> str:
        def _transcribe() -> str:
            result = self._model.transcribe(audio, language="en", fp16=False)
            return str(result.get("text", "")).strip()

        try:
            return await asyncio.to_thread(_transcribe)
        except Exception as exc:
            logger.error("[STT] Whisper failed: %s", exc)
            raise SpeechToTextError(str(exc)) from exc


class TextToSpeech:
    """Optimized TTS with fast providers."""

    def __init__(self, *, language: str, default_speaker: Optional[str] = None) -> None:
        self._language = language
        self._default_speaker = default_speaker
        self._provider = os.getenv("REX_TTS_PROVIDER", "edge").lower()  # Default to edge for speed
        self._edge_voice = os.getenv("REX_TTS_VOICE", "en-US-AndrewNeural")
        
        # Warn if using slow XTTS
        if self._provider == "xtts":
            logger.warning("[TTS] XTTS is slow (~3-4s). Recommend 'edge' or 'windows' for <1s latency")

        self._tts = None
        if self._provider == "xtts" and TTS is not None:
            try:
                import torch
                self._tts = TTS(
                    model_name="tts_models/multilingual/multi-dataset/xtts_v2",
                    progress_bar=False,
                )
                if torch.cuda.is_available():
                    self._tts.to("cuda")
                    logger.info("[TTS] XTTS on CUDA (high quality, ~2s latency)")
                else:
                    logger.info("[TTS] XTTS on CPU (high quality, ~4s latency)")
            except Exception as exc:
                logger.warning("XTTS init failed: %s", exc)
        elif self._provider == "edge":
            logger.info("[TTS] Edge-TTS (cloud, <1s latency, high quality)")
        elif self._provider == "windows":
            logger.info("[TTS] Windows SAPI (instant, robotic)")

    async def speak(self, text: str, *, speaker_wav: Optional[str] = None) -> None:
        if not text:
            return

        text = self._clean_text(text)
        if not text:
            return

        try:
            if self._provider == "xtts":
                await self._speak_xtts(text, speaker_wav)
            elif self._provider == "edge":
                await self._speak_edge(text)
            elif self._provider == "windows":
                await self._speak_windows(text)
            else:
                print(f"Rex: {text}")
        except Exception as exc:
            logger.error("[TTS] Failed: %s", exc)
            print(f"Rex: {text}")

    def _clean_text(self, text: str) -> str:
        # Remove plugin additions
        if "Additional info:" in text:
            text = text.split("Additional info:")[0].strip()

        # Remove URLs
        text = re.sub(r"http[s]?://\S+", "", text)
        text = re.sub(r"uddg=\S+", "", text)
        text = re.sub(r"\[.*?\]", "", text)

        # Limit to first 2 sentences for faster TTS
        sentences = [s.strip() for s in text.split(".") if s.strip()]
        text = ". ".join(sentences[:2])
        if text and not text.endswith("."):
            text += "."
        return text

    async def _speak_xtts(self, text: str, speaker_wav: Optional[str]) -> None:
        if self._tts is None:
            raise TextToSpeechError("XTTS not initialized")

        speaker = speaker_wav or self._default_speaker

        def _synthesise() -> None:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as handle:
                temp_path = Path(handle.name)
            try:
                kwargs = {
                    "text": text,
                    "language": self._language,
                    "file_path": str(temp_path),
                }
                if speaker:
                    kwargs["speaker_wav"] = speaker
                else:
                    kwargs["speaker"] = "Claribel Dervla"

                self._tts.tts_to_file(**kwargs)

                if sa is not None:
                    wave_obj = sa.WaveObject.from_wave_file(str(temp_path))
                    play_obj = wave_obj.play()
                    play_obj.wait_done()
            finally:
                with suppress(FileNotFoundError):
                    temp_path.unlink()

        await asyncio.to_thread(_synthesise)

    async def _speak_edge(self, text: str) -> None:
        try:
            import edge_tts
        except ImportError as exc:
            raise TextToSpeechError("edge-tts not installed. Run: pip install edge-tts") from exc

        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as handle:
            temp_path = Path(handle.name)
        try:
            communicate = edge_tts.Communicate(text, self._edge_voice)
            await communicate.save(str(temp_path))

            if sa is not None:
                # Convert to WAV for playback
                import subprocess
                wav_path = temp_path.with_suffix(".wav")
                subprocess.run(
                    ["ffmpeg", "-i", str(temp_path), "-y", str(wav_path), "-loglevel", "error"],
                    capture_output=True,
                    check=True,
                )

                wave_obj = sa.WaveObject.from_wave_file(str(wav_path))
                play_obj = wave_obj.play()
                play_obj.wait_done()
                wav_path.unlink()
        finally:
            with suppress(FileNotFoundError):
                temp_path.unlink()

    async def _speak_windows(self, text: str) -> None:
        def _speak() -> None:
            try:
                import pyttsx3
            except ImportError as exc:
                raise TextToSpeechError("pyttsx3 not installed. Run: pip install pyttsx3") from exc

            engine = pyttsx3.init()
            engine.setProperty("rate", 180)
            engine.setProperty("volume", 0.9)
            engine.say(text)
            engine.runAndWait()

        await asyncio.to_thread(_speak)


FrameSource = Callable[[], Awaitable[np.ndarray]]
PhraseRecorder = Callable[[], Awaitable[np.ndarray]]
Transcriber = Callable[[np.ndarray], Awaitable[str]]
Speaker = Callable[[str], Awaitable[None]]
Acknowledgement = Callable[[], Awaitable[None]]


class VoiceLoop:
    """Optimized voice loop with reduced latency."""

    def __init__(
        self,
        assistant: Assistant,
        *,
        wake_listener: WakeWordListener,
        detection_source: FrameSource,
        record_phrase: PhraseRecorder,
        transcribe: Transcriber,
        speak: Speaker,
        acknowledge: Optional[Acknowledgement] = None,
    ) -> None:
        self._assistant = assistant
        self._wake_listener = wake_listener
        self._detection_source = detection_source
        self._record_phrase = record_phrase
        self._transcribe = transcribe
        self._speak = speak
        self._acknowledge = acknowledge

    async def run(self, *, max_interactions: Optional[int] = None) -> None:
        interactions = 0
        while max_interactions is None or interactions < max_interactions:
            try:
                await self._await_wakeword()
            except (AudioDeviceError, WakeWordError) as exc:
                logger.error("Wake word error: %s", exc)
                interactions += 1
                await asyncio.sleep(1)
                continue

            interactions += 1

            # Play acknowledgment asynchronously (don't wait)
            if self._acknowledge:
                asyncio.create_task(self._acknowledge())

            # Record audio
            try:
                audio = await self._record_phrase()
            except AudioDeviceError as exc:
                logger.error("Recording failed: %s", exc)
                continue

            # Transcribe audio
            try:
                transcript = await self._transcribe(audio)
                logger.info(f"[STT] '{transcript}'")
            except SpeechToTextError as exc:
                logger.error("Transcription failed: %s", exc)
                continue

            if not transcript.strip():
                continue

            # Generate response
            try:
                response = await self._assistant.generate_reply(transcript)
                logger.info(f"[LLM] '{response}'")
            except Exception as exc:
                logger.error("LLM failed: %s", exc)
                continue

            if not response.strip():
                continue

            # Clean and speak response
            response = self._clean_response(response)
            if response:
                try:
                    await self._speak(response)
                except TextToSpeechError as exc:
                    logger.error("TTS failed: %s", exc)

    async def _await_wakeword(self) -> None:
        try:
            async for _frame in self._wake_listener.listen(self._detection_source):
                self._wake_listener.stop()
                return
        except AudioDeviceError:
            raise
        except Exception as exc:
            raise WakeWordError(str(exc)) from exc

        raise WakeWordError("Wake-word listener exited unexpectedly")

    def _clean_response(self, response: str) -> str:
        """Clean response for TTS."""
        if "Additional info:" in response:
            response = response.split("Additional info:")[0].strip()

        response = re.sub(r"http[s]?://\S+", "", response)
        response = re.sub(r"uddg=\S+", "", response)
        response = re.sub(r"\[.*?\]", "", response)

        sentences = [s.strip() for s in response.split(".") if s.strip()]
        response = ". ".join(sentences[:2]).strip()
        if response and not response.endswith("."):
            response += "."
        
        return response


def _resolve_voice_reference() -> Optional[str]:
    try:
        users_map = load_users_map()
        profiles = load_all_profiles()
        user_key = resolve_user_key(settings.user_id, users_map, profiles=profiles)
        if user_key and user_key in profiles:
            reference = extract_voice_reference(profiles[user_key])
            if reference and Path(reference).exists():
                return reference
    except Exception:
        pass
    return None


def build_voice_loop(assistant: Assistant) -> VoiceLoop:
    """Build optimized voice loop with reduced latency."""
    if sd is not None and (settings.audio_input_device is not None or settings.audio_output_device is not None):
        sd.default.device = (settings.audio_input_device, settings.audio_output_device)

    # Microphone with VAD for faster recording
    microphone = AsyncMicrophone(
        sample_rate=settings.sample_rate,
        detection_seconds=settings.detection_frame_seconds,
        capture_seconds=settings.capture_seconds,
        vad_threshold=0.01,  # Adjust based on your environment
        silence_duration=1.0,  # Stop after 1s of silence
    )

    try:
        wake_model, _ = load_wakeword_model(keyword=settings.wakeword_keyword)
    except Exception as exc:
        raise WakeWordError(str(exc)) from exc

    detector = build_default_detector(wake_model, threshold=settings.wakeword_threshold)
    wake_listener = WakeWordListener(detector, poll_interval=settings.wakeword_poll_interval)
    acknowledgement = WakeAcknowledgement()

    # Speech-to-text (automatically uses 'tiny' model if 'base' is configured)
    speech_to_text: Optional[SpeechToText]
    try:
        speech_to_text = SpeechToText(settings.whisper_model, settings.whisper_device)
    except SpeechToTextError as exc:
        logger.warning("STT unavailable: %s", exc)
        speech_to_text = None

    voice_reference = _resolve_voice_reference()
    text_to_speech = TextToSpeech(language=settings.speak_language, default_speaker=voice_reference)

    async def detection_source() -> np.ndarray:
        return await microphone.detection_frame()

    async def record_phrase() -> np.ndarray:
        audio = await microphone.record_phrase()
        
        # Optional: Save debug recording
        if settings.debug_logging:
            try:
                sf.write("debug_recording.wav", audio, microphone.sample_rate)
            except Exception:
                pass
        
        return audio

    async def transcribe(audio: np.ndarray) -> str:
        if speech_to_text is None:
            raise SpeechToTextError("STT unavailable")
        return await speech_to_text.transcribe(audio, microphone.sample_rate)

    async def speak(text: str) -> None:
        await text_to_speech.speak(text, speaker_wav=voice_reference)

    async def acknowledge() -> None:
        await acknowledgement.play()

    return VoiceLoop(
        assistant,
        wake_listener=wake_listener,
        detection_source=detection_source,
        record_phrase=record_phrase,
        transcribe=transcribe,
        speak=speak,
        acknowledge=acknowledge,
    )


__all__ = [
    "AsyncMicrophone",
    "WakeAcknowledgement",
    "SpeechToText",
    "TextToSpeech",
    "VoiceLoop",
    "build_voice_loop",
]
